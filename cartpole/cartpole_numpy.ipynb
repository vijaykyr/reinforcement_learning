{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym # tested with gym version 0.12.1\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Cartpole\n",
    "\n",
    "Cartpole is a game in which a pole is attached by a joint to a cart, which moves along a frictionless track. The pole starts slightly angled, and the goal is to prevent it from falling over by applying force to the left or right of the cart.\n",
    "\n",
    "If you can stay upright (+/- 12 degrees) for 200 time steps you win!\n",
    "\n",
    "The game is considered solved by an AI agent when the average reward is greater than or equal to 195 over 100 consecutive trials.\n",
    "\n",
    "For more details about the game see the [game wiki](https://github.com/openai/gym/wiki/CartPole-v0) and you can play it online [here](https://fluxml.ai/experiments/cartPole/). It's surprisingly difficult!\n",
    "\n",
    "<img src='assets/cartpole.gif' width='25%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Open AI Gym\n",
    " \n",
    "[Gym](https://gym.openai.com/) is a python framework for developing and comparing reinforcement learning algorithms. In addition to Cartpole it provides several environments including Atari games like Pong and Pacman. It provides a simple interface which takes in outputs from an AI agent, and updates the environment accordingly. Leaving you to just focus on the AI agent itself!\n",
    "\n",
    "To instantiate an environment simply use `gym.make()` and provide the name of a supported environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent\n",
    "\n",
    "Here's a demonstration of a very simple AI agent. The agent uses the environment's `action_space.sample()` method which simply selects an action at random from the space of possible actions (just 'left' or 'right' for the cartpole environment).\n",
    "\n",
    "The action is then fed to the environment using using `env.step()` which returns four values:\n",
    "- **observation** (object): an environment-specific object representing the new state of the environment after taking the specified action\n",
    "- **reward** (float): amount of reward achieved by the previous action\n",
    "- **done** (boolean): indicates whether the episode has terminated because you won or lost\n",
    "- **info** (dict): diagnostic information useful for debugging\n",
    "\n",
    "At any point we can visualize the current state of the environment by calling `env.render()`\n",
    "\n",
    "For more information on the gym API see the official [documentation](https://gym.openai.com/docs/).\n",
    "\n",
    "This 'agent-environment' loop continues until the the episode terminates, at which point we reset the environment and start the loop again. This loop is visualized below:\n",
    "\n",
    "<img src=\"assets/agent-environment-loop.png\" alt=\"Agent Envirnoment Loop\">\n",
    "<sub>Image Credit: *Reinforcement Learning: An Introduction 2nd Edition, Richard S. Sutton and Andrew G. Barto*</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward 13.0\n",
      "Episode 2 finished with reward 16.0\n",
      "Episode 3 finished with reward 28.0\n",
      "Episode 4 finished with reward 57.0\n",
      "Episode 5 finished with reward 16.0\n",
      "Episode 6 finished with reward 11.0\n",
      "Episode 7 finished with reward 15.0\n",
      "Episode 8 finished with reward 23.0\n",
      "Episode 9 finished with reward 27.0\n",
      "Episode 10 finished with reward 27.0\n",
      "Episode 11 finished with reward 131.0\n",
      "Episode 12 finished with reward 19.0\n",
      "Episode 13 finished with reward 23.0\n",
      "Episode 14 finished with reward 24.0\n",
      "Episode 15 finished with reward 20.0\n",
      "Episode 16 finished with reward 24.0\n",
      "Episode 17 finished with reward 16.0\n",
      "Episode 18 finished with reward 22.0\n",
      "Episode 19 finished with reward 25.0\n",
      "Episode 20 finished with reward 11.0\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES=20\n",
    "for i in range(20):\n",
    "  env.reset()\n",
    "  reward_sum = 0\n",
    "  while True:\n",
    "    #env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "      print(\"Episode {} finished with reward {}\".format(i+1,reward_sum))\n",
    "      break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "\n",
    "Clearly that agent isn't going to get any better with time.\n",
    "\n",
    "Now instead of sampling our action randomly, let's have our action be selected by a neural network.\n",
    "\n",
    "It's input will be the observation object (state) for a given timestep, and it's output will be a number between 0 and 1 which we will interpret as the probability of applying force to the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    return 1/(1+np.exp(-a)) # squashes a between 0 and 1\n",
    "\n",
    "def policy_network(observation,model):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      observation: a float vector describing the environment's state\n",
    "      model: a dictionary containing weight matrices 'W1','W2'\n",
    "    Returns: probabilty of action '1', h\n",
    "    \"\"\"\n",
    "    observation = np.reshape(observation,[1,len(observation)])\n",
    "    h = np.matmul(observation,model['W1'])\n",
    "    h[h<0] = 0 # relu \n",
    "    logit_p = np.matmul(h,model['W2'])\n",
    "    return sigmoid(logit_p), h # h is returned because it will be used in backprop calculation later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Learning Happens\n",
    "\n",
    "So we have our inputs and our model which allows to make predictions. But of course we don't have labels, which is what makes this a reinforcment learning problem and not a supervised learning problem.\n",
    "\n",
    "So how do we know how to update our model weights to improve performance?\n",
    "\n",
    "We *engineer* labels based on some policy. \n",
    "\n",
    "Our policy will be so simple it's surprising how well it works. \n",
    "\n",
    "- If the episode ends in failure, assume every action in the episode was bad. \n",
    "- If the episode ends in success, assume every action in the episode was good\n",
    "\n",
    "We will make one improvement to this policy:\n",
    "\n",
    "- Assume recent actions are more responsible for success/failure than earlier action\n",
    "\n",
    "Mathematically we can accomplish this by exponentially decaying as in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_reward(rewards,decay_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      rewards: list of numbers\n",
    "      decay_rate: rate of exponential decay\n",
    "    Returns: exponentially decaying list\n",
    "    \"\"\"\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[-i-1] = rewards[-i-1]*(decay_rate**i)\n",
    "    return rewards    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Inner loop (agent-environment loop):\n",
    "\n",
    "1. Give state to agent, get action   \n",
    "2. Give action to environment, get new state\n",
    "3. Repeat until episode terminates \n",
    "\n",
    "Outer loop (gradient descent loop):\n",
    "\n",
    "1. Engineer labels for each action in the episode based on whether it ended in success or failure\n",
    "2. Back Propogate to calculate gradients\n",
    "3. Update weights\n",
    "\n",
    "Note how similar this is to supervised learning. In fact **it is** supervised learning, with just two differences!\n",
    "\n",
    "1. We have to wait till an episode terminates to get our labels\n",
    "2. Our labels are inferred from whether an episode fails or succeeds, as opposed to being 'ground truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward 30.0. Running Avg 30.0\n",
      "Episode 2 finished with reward 12.0. Running Avg 21.0\n",
      "Episode 3 finished with reward 17.0. Running Avg 19.666666666666668\n",
      "Episode 4 finished with reward 12.0. Running Avg 17.75\n",
      "Episode 5 finished with reward 12.0. Running Avg 16.6\n",
      "Episode 6 finished with reward 45.0. Running Avg 21.333333333333332\n",
      "Episode 7 finished with reward 42.0. Running Avg 24.285714285714285\n",
      "Episode 8 finished with reward 32.0. Running Avg 25.25\n",
      "Episode 9 finished with reward 30.0. Running Avg 25.77777777777778\n",
      "Episode 10 finished with reward 19.0. Running Avg 25.1\n",
      "Episode 11 finished with reward 9.0. Running Avg 23.636363636363637\n",
      "Episode 12 finished with reward 30.0. Running Avg 24.166666666666668\n",
      "Episode 13 finished with reward 45.0. Running Avg 25.76923076923077\n",
      "Episode 14 finished with reward 32.0. Running Avg 26.214285714285715\n",
      "Episode 15 finished with reward 44.0. Running Avg 27.4\n",
      "Episode 16 finished with reward 21.0. Running Avg 27.0\n",
      "Episode 17 finished with reward 84.0. Running Avg 30.352941176470587\n",
      "Episode 18 finished with reward 32.0. Running Avg 30.444444444444443\n",
      "Episode 19 finished with reward 55.0. Running Avg 31.736842105263158\n",
      "Episode 20 finished with reward 49.0. Running Avg 32.6\n",
      "Episode 21 finished with reward 37.0. Running Avg 32.80952380952381\n",
      "Episode 22 finished with reward 107.0. Running Avg 36.18181818181818\n",
      "Episode 23 finished with reward 74.0. Running Avg 37.82608695652174\n",
      "Episode 24 finished with reward 89.0. Running Avg 39.958333333333336\n",
      "Episode 25 finished with reward 58.0. Running Avg 40.68\n",
      "Episode 26 finished with reward 51.0. Running Avg 41.07692307692308\n",
      "Episode 27 finished with reward 73.0. Running Avg 42.25925925925926\n",
      "Episode 28 finished with reward 86.0. Running Avg 43.82142857142857\n",
      "Episode 29 finished with reward 60.0. Running Avg 44.37931034482759\n",
      "Episode 30 finished with reward 41.0. Running Avg 44.266666666666666\n",
      "Episode 31 finished with reward 124.0. Running Avg 46.83870967741935\n",
      "Episode 32 finished with reward 103.0. Running Avg 48.59375\n",
      "Episode 33 finished with reward 126.0. Running Avg 50.93939393939394\n",
      "Episode 34 finished with reward 81.0. Running Avg 51.8235294117647\n",
      "Episode 35 finished with reward 43.0. Running Avg 51.57142857142857\n",
      "Episode 36 finished with reward 145.0. Running Avg 54.166666666666664\n",
      "Episode 37 finished with reward 52.0. Running Avg 54.108108108108105\n",
      "Episode 38 finished with reward 73.0. Running Avg 54.60526315789474\n",
      "Episode 39 finished with reward 54.0. Running Avg 54.58974358974359\n",
      "Episode 40 finished with reward 107.0. Running Avg 55.9\n",
      "Episode 41 finished with reward 84.0. Running Avg 56.58536585365854\n",
      "Episode 42 finished with reward 101.0. Running Avg 57.642857142857146\n",
      "Episode 43 finished with reward 57.0. Running Avg 57.627906976744185\n",
      "Episode 44 finished with reward 86.0. Running Avg 58.27272727272727\n",
      "Episode 45 finished with reward 52.0. Running Avg 58.13333333333333\n",
      "Episode 46 finished with reward 75.0. Running Avg 58.5\n",
      "Episode 47 finished with reward 158.0. Running Avg 60.61702127659574\n",
      "Episode 48 finished with reward 200.0. Running Avg 63.520833333333336\n",
      "Episode 49 finished with reward 65.0. Running Avg 63.55102040816327\n",
      "Episode 50 finished with reward 74.0. Running Avg 63.76\n",
      "Episode 51 finished with reward 200.0. Running Avg 66.43137254901961\n",
      "Episode 52 finished with reward 181.0. Running Avg 68.63461538461539\n",
      "Episode 53 finished with reward 200.0. Running Avg 71.11320754716981\n",
      "Episode 54 finished with reward 95.0. Running Avg 71.55555555555556\n",
      "Episode 55 finished with reward 119.0. Running Avg 72.41818181818182\n",
      "Episode 56 finished with reward 169.0. Running Avg 74.14285714285714\n",
      "Episode 57 finished with reward 108.0. Running Avg 74.73684210526316\n",
      "Episode 58 finished with reward 73.0. Running Avg 74.70689655172414\n",
      "Episode 59 finished with reward 200.0. Running Avg 76.83050847457628\n",
      "Episode 60 finished with reward 200.0. Running Avg 78.88333333333334\n",
      "Episode 61 finished with reward 200.0. Running Avg 80.8688524590164\n",
      "Episode 62 finished with reward 200.0. Running Avg 82.79032258064517\n",
      "Episode 63 finished with reward 158.0. Running Avg 83.98412698412699\n",
      "Episode 64 finished with reward 169.0. Running Avg 85.3125\n",
      "Episode 65 finished with reward 156.0. Running Avg 86.4\n",
      "Episode 66 finished with reward 135.0. Running Avg 87.13636363636364\n",
      "Episode 67 finished with reward 106.0. Running Avg 87.41791044776119\n",
      "Episode 68 finished with reward 200.0. Running Avg 89.07352941176471\n",
      "Episode 69 finished with reward 200.0. Running Avg 90.68115942028986\n",
      "Episode 70 finished with reward 200.0. Running Avg 92.24285714285715\n",
      "Episode 71 finished with reward 127.0. Running Avg 92.73239436619718\n",
      "Episode 72 finished with reward 174.0. Running Avg 93.86111111111111\n",
      "Episode 73 finished with reward 64.0. Running Avg 93.45205479452055\n",
      "Episode 74 finished with reward 88.0. Running Avg 93.37837837837837\n",
      "Episode 75 finished with reward 35.0. Running Avg 92.6\n",
      "Episode 76 finished with reward 73.0. Running Avg 92.34210526315789\n",
      "Episode 77 finished with reward 134.0. Running Avg 92.88311688311688\n",
      "Episode 78 finished with reward 172.0. Running Avg 93.8974358974359\n",
      "Episode 79 finished with reward 136.0. Running Avg 94.43037974683544\n",
      "Episode 80 finished with reward 79.0. Running Avg 94.2375\n",
      "Episode 81 finished with reward 73.0. Running Avg 93.9753086419753\n",
      "Episode 82 finished with reward 167.0. Running Avg 94.86585365853658\n",
      "Episode 83 finished with reward 200.0. Running Avg 96.13253012048193\n",
      "Episode 84 finished with reward 144.0. Running Avg 96.70238095238095\n",
      "Episode 85 finished with reward 181.0. Running Avg 97.69411764705882\n",
      "Episode 86 finished with reward 160.0. Running Avg 98.4186046511628\n",
      "Episode 87 finished with reward 200.0. Running Avg 99.58620689655173\n",
      "Episode 88 finished with reward 106.0. Running Avg 99.6590909090909\n",
      "Episode 89 finished with reward 109.0. Running Avg 99.76404494382022\n",
      "Episode 90 finished with reward 102.0. Running Avg 99.78888888888889\n",
      "Episode 91 finished with reward 71.0. Running Avg 99.47252747252747\n",
      "Episode 92 finished with reward 115.0. Running Avg 99.6413043478261\n",
      "Episode 93 finished with reward 63.0. Running Avg 99.24731182795699\n",
      "Episode 94 finished with reward 100.0. Running Avg 99.25531914893617\n",
      "Episode 95 finished with reward 75.0. Running Avg 99.0\n",
      "Episode 96 finished with reward 43.0. Running Avg 98.41666666666667\n",
      "Episode 97 finished with reward 71.0. Running Avg 98.1340206185567\n",
      "Episode 98 finished with reward 65.0. Running Avg 97.79591836734694\n",
      "Episode 99 finished with reward 117.0. Running Avg 97.98989898989899\n",
      "Episode 100 finished with reward 200.0. Running Avg 99.01\n",
      "Episode 101 finished with reward 94.0. Running Avg 99.65\n",
      "Episode 102 finished with reward 200.0. Running Avg 101.53\n",
      "Episode 103 finished with reward 200.0. Running Avg 103.36\n",
      "Episode 104 finished with reward 200.0. Running Avg 105.24\n",
      "Episode 105 finished with reward 200.0. Running Avg 107.12\n",
      "Episode 106 finished with reward 200.0. Running Avg 108.67\n",
      "Episode 107 finished with reward 200.0. Running Avg 110.25\n",
      "Episode 108 finished with reward 200.0. Running Avg 111.93\n",
      "Episode 109 finished with reward 164.0. Running Avg 113.27\n",
      "Episode 110 finished with reward 102.0. Running Avg 114.1\n",
      "Episode 111 finished with reward 159.0. Running Avg 115.6\n",
      "Episode 112 finished with reward 47.0. Running Avg 115.77\n",
      "Episode 113 finished with reward 68.0. Running Avg 116.0\n",
      "Episode 114 finished with reward 84.0. Running Avg 116.52\n",
      "Episode 115 finished with reward 104.0. Running Avg 117.12\n",
      "Episode 116 finished with reward 65.0. Running Avg 117.56\n",
      "Episode 117 finished with reward 106.0. Running Avg 117.78\n",
      "Episode 118 finished with reward 63.0. Running Avg 118.09\n",
      "Episode 119 finished with reward 88.0. Running Avg 118.42\n",
      "Episode 120 finished with reward 115.0. Running Avg 119.08\n",
      "Episode 121 finished with reward 54.0. Running Avg 119.25\n",
      "Episode 122 finished with reward 88.0. Running Avg 119.06\n",
      "Episode 123 finished with reward 90.0. Running Avg 119.22\n",
      "Episode 124 finished with reward 88.0. Running Avg 119.21\n",
      "Episode 125 finished with reward 105.0. Running Avg 119.68\n",
      "Episode 126 finished with reward 108.0. Running Avg 120.25\n",
      "Episode 127 finished with reward 107.0. Running Avg 120.59\n",
      "Episode 128 finished with reward 57.0. Running Avg 120.3\n",
      "Episode 129 finished with reward 62.0. Running Avg 120.32\n",
      "Episode 130 finished with reward 113.0. Running Avg 121.04\n",
      "Episode 131 finished with reward 121.0. Running Avg 121.01\n",
      "Episode 132 finished with reward 134.0. Running Avg 121.32\n",
      "Episode 133 finished with reward 54.0. Running Avg 120.6\n",
      "Episode 134 finished with reward 77.0. Running Avg 120.56\n",
      "Episode 135 finished with reward 69.0. Running Avg 120.82\n",
      "Episode 136 finished with reward 89.0. Running Avg 120.26\n",
      "Episode 137 finished with reward 146.0. Running Avg 121.2\n",
      "Episode 138 finished with reward 98.0. Running Avg 121.45\n",
      "Episode 139 finished with reward 168.0. Running Avg 122.59\n",
      "Episode 140 finished with reward 187.0. Running Avg 123.39\n",
      "Episode 141 finished with reward 96.0. Running Avg 123.51\n",
      "Episode 142 finished with reward 118.0. Running Avg 123.68\n",
      "Episode 143 finished with reward 188.0. Running Avg 124.99\n",
      "Episode 144 finished with reward 175.0. Running Avg 125.88\n",
      "Episode 145 finished with reward 132.0. Running Avg 126.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 146 finished with reward 200.0. Running Avg 127.93\n",
      "Episode 147 finished with reward 200.0. Running Avg 128.35\n",
      "Episode 148 finished with reward 200.0. Running Avg 128.35\n",
      "Episode 149 finished with reward 156.0. Running Avg 129.26\n",
      "Episode 150 finished with reward 200.0. Running Avg 130.52\n",
      "Episode 151 finished with reward 200.0. Running Avg 130.52\n",
      "Episode 152 finished with reward 109.0. Running Avg 129.8\n",
      "Episode 153 finished with reward 172.0. Running Avg 129.52\n",
      "Episode 154 finished with reward 200.0. Running Avg 130.57\n",
      "Episode 155 finished with reward 200.0. Running Avg 131.38\n",
      "Episode 156 finished with reward 168.0. Running Avg 131.37\n",
      "Episode 157 finished with reward 153.0. Running Avg 131.82\n",
      "Episode 158 finished with reward 200.0. Running Avg 133.09\n",
      "Episode 159 finished with reward 200.0. Running Avg 133.09\n",
      "Episode 160 finished with reward 200.0. Running Avg 133.09\n",
      "Episode 161 finished with reward 200.0. Running Avg 133.09\n",
      "Episode 162 finished with reward 200.0. Running Avg 133.09\n",
      "Episode 163 finished with reward 200.0. Running Avg 133.51\n",
      "Episode 164 finished with reward 200.0. Running Avg 133.82\n",
      "Episode 165 finished with reward 200.0. Running Avg 134.26\n",
      "Episode 166 finished with reward 200.0. Running Avg 134.91\n",
      "Episode 167 finished with reward 200.0. Running Avg 135.85\n",
      "Episode 168 finished with reward 200.0. Running Avg 135.85\n",
      "Episode 169 finished with reward 101.0. Running Avg 134.86\n",
      "Episode 170 finished with reward 192.0. Running Avg 134.78\n",
      "Episode 171 finished with reward 129.0. Running Avg 134.8\n",
      "Episode 172 finished with reward 200.0. Running Avg 135.06\n",
      "Episode 173 finished with reward 200.0. Running Avg 136.42\n",
      "Episode 174 finished with reward 171.0. Running Avg 137.25\n",
      "Episode 175 finished with reward 200.0. Running Avg 138.9\n",
      "Episode 176 finished with reward 200.0. Running Avg 140.17\n",
      "Episode 177 finished with reward 200.0. Running Avg 140.83\n",
      "Episode 178 finished with reward 200.0. Running Avg 141.11\n",
      "Episode 179 finished with reward 200.0. Running Avg 141.75\n",
      "Episode 180 finished with reward 200.0. Running Avg 142.96\n",
      "Episode 181 finished with reward 200.0. Running Avg 144.23\n",
      "Episode 182 finished with reward 200.0. Running Avg 144.56\n",
      "Episode 183 finished with reward 200.0. Running Avg 144.56\n",
      "Episode 184 finished with reward 200.0. Running Avg 145.12\n",
      "Episode 185 finished with reward 200.0. Running Avg 145.31\n",
      "Episode 186 finished with reward 200.0. Running Avg 145.71\n",
      "Episode 187 finished with reward 200.0. Running Avg 145.71\n",
      "Episode 188 finished with reward 200.0. Running Avg 146.65\n",
      "Episode 189 finished with reward 200.0. Running Avg 147.56\n",
      "Episode 190 finished with reward 200.0. Running Avg 148.54\n",
      "Episode 191 finished with reward 200.0. Running Avg 149.83\n",
      "Episode 192 finished with reward 200.0. Running Avg 150.68\n",
      "Episode 193 finished with reward 200.0. Running Avg 152.05\n",
      "Episode 194 finished with reward 200.0. Running Avg 153.05\n",
      "Episode 195 finished with reward 200.0. Running Avg 154.3\n",
      "Episode 196 finished with reward 200.0. Running Avg 155.87\n",
      "Episode 197 finished with reward 158.0. Running Avg 156.74\n",
      "Episode 198 finished with reward 145.0. Running Avg 157.54\n",
      "Episode 199 finished with reward 200.0. Running Avg 158.37\n",
      "Episode 200 finished with reward 200.0. Running Avg 158.37\n",
      "Episode 201 finished with reward 177.0. Running Avg 159.2\n",
      "Episode 202 finished with reward 177.0. Running Avg 158.97\n",
      "Episode 203 finished with reward 147.0. Running Avg 158.44\n",
      "Episode 204 finished with reward 200.0. Running Avg 158.44\n",
      "Episode 205 finished with reward 200.0. Running Avg 158.44\n",
      "Episode 206 finished with reward 200.0. Running Avg 158.44\n",
      "Episode 207 finished with reward 200.0. Running Avg 158.44\n",
      "Episode 208 finished with reward 200.0. Running Avg 158.44\n",
      "Episode 209 finished with reward 200.0. Running Avg 158.8\n",
      "Episode 210 finished with reward 186.0. Running Avg 159.64\n",
      "Episode 211 finished with reward 200.0. Running Avg 160.05\n",
      "Episode 212 finished with reward 200.0. Running Avg 161.58\n",
      "Episode 213 finished with reward 200.0. Running Avg 162.9\n",
      "Episode 214 finished with reward 160.0. Running Avg 163.66\n",
      "Episode 215 finished with reward 200.0. Running Avg 164.62\n",
      "Episode 216 finished with reward 200.0. Running Avg 165.97\n",
      "Episode 217 finished with reward 200.0. Running Avg 166.91\n",
      "Episode 218 finished with reward 142.0. Running Avg 167.7\n",
      "Episode 219 finished with reward 200.0. Running Avg 168.82\n",
      "Episode 220 finished with reward 200.0. Running Avg 169.67\n",
      "Episode 221 finished with reward 200.0. Running Avg 171.13\n",
      "Episode 222 finished with reward 200.0. Running Avg 172.25\n",
      "Episode 223 finished with reward 200.0. Running Avg 173.35\n",
      "Episode 224 finished with reward 200.0. Running Avg 174.47\n",
      "Episode 225 finished with reward 200.0. Running Avg 175.42\n",
      "Episode 226 finished with reward 195.0. Running Avg 176.29\n",
      "Episode 227 finished with reward 200.0. Running Avg 177.22\n",
      "Episode 228 finished with reward 200.0. Running Avg 178.65\n",
      "Episode 229 finished with reward 200.0. Running Avg 180.03\n",
      "Episode 230 finished with reward 200.0. Running Avg 180.9\n",
      "Episode 231 finished with reward 200.0. Running Avg 181.69\n",
      "Episode 232 finished with reward 200.0. Running Avg 182.35\n",
      "Episode 233 finished with reward 200.0. Running Avg 183.81\n",
      "Episode 234 finished with reward 200.0. Running Avg 185.04\n",
      "Episode 235 finished with reward 200.0. Running Avg 186.35\n",
      "Episode 236 finished with reward 200.0. Running Avg 187.46\n",
      "Episode 237 finished with reward 200.0. Running Avg 188.0\n",
      "Episode 238 finished with reward 200.0. Running Avg 189.02\n",
      "Episode 239 finished with reward 200.0. Running Avg 189.34\n",
      "Episode 240 finished with reward 200.0. Running Avg 189.47\n",
      "Episode 241 finished with reward 200.0. Running Avg 190.51\n",
      "Episode 242 finished with reward 200.0. Running Avg 191.33\n",
      "Episode 243 finished with reward 200.0. Running Avg 191.45\n",
      "Episode 244 finished with reward 200.0. Running Avg 191.7\n",
      "Episode 245 finished with reward 200.0. Running Avg 192.38\n",
      "Episode 246 finished with reward 200.0. Running Avg 192.38\n",
      "Episode 247 finished with reward 200.0. Running Avg 192.38\n",
      "Episode 248 finished with reward 200.0. Running Avg 192.38\n",
      "Episode 249 finished with reward 200.0. Running Avg 192.82\n",
      "Episode 250 finished with reward 200.0. Running Avg 192.82\n",
      "Episode 251 finished with reward 200.0. Running Avg 192.82\n",
      "Episode 252 finished with reward 200.0. Running Avg 193.73\n",
      "Episode 253 finished with reward 149.0. Running Avg 193.5\n",
      "Episode 254 finished with reward 200.0. Running Avg 193.5\n",
      "Episode 255 finished with reward 200.0. Running Avg 193.5\n",
      "Episode 256 finished with reward 200.0. Running Avg 193.82\n",
      "Episode 257 finished with reward 200.0. Running Avg 194.29\n",
      "Episode 258 finished with reward 200.0. Running Avg 194.29\n",
      "Episode 259 finished with reward 200.0. Running Avg 194.29\n",
      "Episode 260 finished with reward 200.0. Running Avg 194.29\n",
      "Episode 261 finished with reward 200.0. Running Avg 194.29\n",
      "Episode 262 finished with reward 200.0. Running Avg 194.29\n",
      "Episode 263 finished with reward 200.0. Running Avg 194.29\n",
      "Episode 264 finished with reward 130.0. Running Avg 193.59\n",
      "Episode 265 finished with reward 200.0. Running Avg 193.59\n",
      "Episode 266 finished with reward 200.0. Running Avg 193.59\n",
      "Episode 267 finished with reward 200.0. Running Avg 193.59\n",
      "Episode 268 finished with reward 136.0. Running Avg 192.95\n",
      "Episode 269 finished with reward 200.0. Running Avg 193.94\n",
      "Episode 270 finished with reward 200.0. Running Avg 194.02\n",
      "Episode 271 finished with reward 200.0. Running Avg 194.73\n",
      "Episode 272 finished with reward 200.0. Running Avg 194.73\n",
      "Episode 273 finished with reward 128.0. Running Avg 194.01\n",
      "Episode 274 finished with reward 200.0. Running Avg 194.3\n",
      "Episode 275 finished with reward 187.0. Running Avg 194.17\n",
      "Episode 276 finished with reward 200.0. Running Avg 194.17\n",
      "Episode 277 finished with reward 200.0. Running Avg 194.17\n",
      "Episode 278 finished with reward 200.0. Running Avg 194.17\n",
      "Episode 279 finished with reward 200.0. Running Avg 194.17\n",
      "Episode 280 finished with reward 200.0. Running Avg 194.17\n",
      "Episode 281 finished with reward 200.0. Running Avg 194.17\n",
      "Episode 282 finished with reward 173.0. Running Avg 193.9\n",
      "Episode 283 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 284 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 285 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 286 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 287 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 288 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 289 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 290 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 291 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 292 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 293 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 294 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 295 finished with reward 200.0. Running Avg 193.9\n",
      "Episode 296 finished with reward 200.0. Running Avg 193.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 297 finished with reward 185.0. Running Avg 194.17\n",
      "Episode 298 finished with reward 200.0. Running Avg 194.72\n",
      "Episode 299 finished with reward 200.0. Running Avg 194.72\n",
      "Episode 300 finished with reward 200.0. Running Avg 194.72\n",
      "Episode 301 finished with reward 200.0. Running Avg 194.95\n",
      "Episode 302 finished with reward 176.0. Running Avg 194.94\n",
      "Episode 303 finished with reward 200.0. Running Avg 195.47\n",
      "CPU times: user 2.33 s, sys: 324 ms, total: 2.65 s\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameters\n",
    "LR = 1e-1 # learning rate\n",
    "HIDDEN_UNITS = 25\n",
    "SEED=0 # for reproducibility\n",
    "MAX_NUM_EPISODES = 5000\n",
    "N_INPUTS = 4\n",
    "RENDER=False\n",
    "DECAY_RATE = .9\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(SEED)\n",
    "scores = deque(maxlen=100) # only track 100 most recent scores\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(SEED)\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(N_INPUTS,HIDDEN_UNITS)/np.sqrt(N_INPUTS) # xavier initialization\n",
    "model['W2'] = np.random.randn(HIDDEN_UNITS,1)/np.sqrt(HIDDEN_UNITS) # xavier initialization\n",
    "\n",
    "for i in range(MAX_NUM_EPISODES):\n",
    "  # Reset environment\n",
    "  observation = env.reset()\n",
    "  reward_sum = 0\n",
    "  episode_observations = [] # collect observations for episode\n",
    "  episode_actions_p = [] # collect action probabilities for episode\n",
    "  episode_actions = [] # action is also assumed label\n",
    "  episode_h = [] # cache for backprop\n",
    "  \n",
    "  if i >= 100 and np.mean(scores) >= 195: break # solved!\n",
    "\n",
    "  while True:\n",
    "    if RENDER: env.render()\n",
    "        \n",
    "    # Give state to agent, get action  \n",
    "    action_p, h = policy_network(observation,model) # return action probability\n",
    "    action = 0 if np.random.uniform() > action_p else 1 # flip biased coin to get action (this bit of stochasticity helps with environment exploration)\n",
    "    \n",
    "    # Store values for this time step\n",
    "    episode_observations.append(observation)\n",
    "    episode_actions_p.append(action_p)\n",
    "    episode_actions.append(action)\n",
    "    episode_h.append(h) # cache h for manual backprop later\n",
    "    \n",
    "    # Give action to environment, get new state and reward\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # When episode terminates...\n",
    "    if done: \n",
    "      # Convert python lists into matrices\n",
    "      episode_observations = np.vstack(episode_observations) # EPISODE_LENGTHxN_INPUTS\n",
    "      episode_actions = np.vstack(episode_actions) # EPISODE_LENGTHx1\n",
    "      episode_actions_p = np.vstack(episode_actions_p) # EPISODE_LENGTHx1\n",
    "      episode_h = np.vstack(episode_h) # EPISODE_LENGTHxHIDDEN_UNITS\n",
    "    \n",
    "      # if action is 0 want to nudge down, if action is 1 nudge up\n",
    "      improve_directions = (episode_actions - episode_actions_p) \n",
    "      \n",
    "      # Exponentially Decay Rewards so that most recent actions are blamed more\n",
    "      improve_directions = discount_reward(improve_directions,DECAY_RATE) \n",
    "      \n",
    "      # Back Propogation \n",
    "      dw2 = np.sum(h*improve_directions,axis=0,keepdims=True).T # HIDDEN_UNITSx1\n",
    "      dh = np.tile(model['W2'].T,[len(episode_observations),1])*improve_directions # EPISODE_LENGTHxHIDDEN_UNITS\n",
    "      dh[np.array(episode_h)<=0] = 0 # relu backprop\n",
    "      dw1 = np.matmul(episode_observations.T,dh) # (N_INPUTS,EPISODE_LENGTH) x (EPISODE_LENGTH,HIDDEN_UNITS) = (N_INPUTS,HIDDEN_UNITS)\n",
    "    \n",
    "      # Weight Updates\n",
    "      if reward_sum == 200: \n",
    "          # do more of what worked\n",
    "          model['W1'] += dw1*LR\n",
    "          model['W2'] += dw2*LR\n",
    "      else: \n",
    "          # do less of what didn't\n",
    "          model['W1'] -= dw1*LR\n",
    "          model['W2'] -= dw2*LR\n",
    "        \n",
    "      # Log progress\n",
    "      scores.append(reward_sum) # dequeue object only keeps last 100 scores\n",
    "      print(\"Episode {} finished with reward {}. Running Avg {}\".format(i+1,reward_sum,np.mean(scores)))\n",
    "      \n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "You may be surpised at how quickly this learns, considering it starts with random weights and doesn't get any positive reward until it 'stumbles upon' a victory.\n",
    "\n",
    "However keep in mind that just because it isn't winning doesn't mean it isn't learning. With every failure it's learning as well. RL can be summed up as follows:\n",
    "\n",
    "*Try something. If it works, do more of it in the future (update weights to make that outcome more likely). If it doesn't work, do less of it in the future (update weights to make that outcome less likely).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "- Make tf.keras version\n",
    "- Manual backprop is fine for a single layer network, but if you want to experiment with more complex policy networks auto-backprop is almost a neccesity.\n",
    "- Use model.predict, engineer labels, then use model.fit (predicting twice so may slow down a bit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
